
@article{MeeEA14,
  timestamp = {2018-04-06T10:17:11Z},
  title = {Climate {{Model Intercomparisons}}: {{Preparing}} for the {{Next Phase}}},
  volume = {95},
  shorttitle = {Climate {{Model Intercomparisons}}},
  number = {9},
  urldate = {2014-05-28},
  journal = {Eos, Transactions American Geophysical Union},
  author = {Meehl, Gerald A. and Moss, Richard and Taylor, Karl E. and Eyring, Veronika and Stouffer, Ronald J. and Bony, Sandrine and Stevens, Bjorn},
  year = {2014},
  keywords = {1622 Earth system modeling,1626 Global climate models,1627 Coupled models of the climate system,bdec15,climate modeling,CMIP,cmip6,easc2018,model intercomparison,onblog},
  pages = {77--78},
  file = {eost2014EO090001.pdf:/Users/BNL28/zotero-storage/storage/6GFCM3UI/eost2014EO090001.pdf:application/pdf;Snapshot:/Users/BNL28/zotero-storage/storage/VXR5R979/abstract.html:text/html}
}

@article{MizEA09,
  timestamp = {2018-04-06T10:28:21Z},
  title = {High-Resolution Global Climate Modelling: The {{UPSCALE}} Project, a Large-Simulation Campaign},
  volume = {7},
  issn = {1991-9603},
  shorttitle = {High-Resolution Global Climate Modelling},
  doi = {10.5194/gmd-7-1629-2014},
  abstract = {The UPSCALE (UK on PRACE: weather-resolving Simulations of Climate for globAL Environmental risk) project constructed and ran an ensemble of HadGEM3 (Hadley Centre Global Environment Model 3) atmosphere-only global climate simulations over the period 1985\textendash{}2011, at resolutions of N512 (25 km), N216 (60 km) and N96 (130 km) as used in current global weather forecasting, seasonal prediction and climate modelling respectively.  Alongside these present climate simulations a parallel ensemble looking at extremes of future climate was run, using a time-slice methodology to consider conditions at the end of this century.
 These simulations were primarily performed using a 144 million core hour, single year grant of computing time from PRACE (the Partnership for Advanced Computing in Europe) in 2012, with additional resources supplied by the Natural Environment  Research Council (NERC) and the Met Office.  Almost 400 terabytes of simulation data were generated on the HERMIT supercomputer at the High Performance Computing Center Stuttgart (HLRS), and transferred to the JASMIN super-data cluster provided by the Science and Technology Facilities Council Centre for Data Archival (STFC CEDA) for analysis and storage.
 In this paper we describe the implementation of the project, present the technical challenges in terms of optimisation, data output, transfer and storage that such a project involves and include details of the model configuration and the composition of the UPSCALE data set.  This data set is available for scientific analysis to allow assessment of the value of model resolution in both present and potential future climate conditions.},
  number = {4},
  urldate = {2014-10-13},
  journal = {Geosci. Model Dev.},
  author = {Mizielinski, M. S. and Roberts, M. J. and Vidale, P. L. and Schiemann, R. and Demory, M.-E. and Strachan, J. and Edwards, T. and Stephens, A. and Lawrence, B. N. and Pritchard, M. and Chiu, P. and Iwi, A. and Churchill, J. and {del Cano Novales}, C. and Kettleborough, J. and Roseblade, W. and Selwood, P. and Foster, M. and Glover, M. and Malcolm, A.},
  month = aug,
  year = {2014},
  keywords = {bnl,bnl-cv,bnl-cv-refgen,bnl-cv-refjnl,cms,easc2018,jasmin13a,resolution},
  pages = {1629--1640},
  file = {Geosci. Model Dev. PDF:/Users/BNL28/zotero-storage/storage/2PDVAPPJ/Mizielinski et al. - 2014 - High-resolution global climate modelling the UPSC.pdf:application/pdf;Geosci. Model Dev. Snapshot:/Users/BNL28/zotero-storage/storage/WPN7BTMW/2014.html:text/html}
}

@article{BesEA16,
  timestamp = {2018-04-05T16:14:20Z},
  title = {Development of a Probabilistic Ocean Modelling System Based on {{NEMO}} 3.5: Application at Eddying Resolution},
  issn = {1991-962X},
  shorttitle = {Development of a Probabilistic Ocean Modelling System Based on {{NEMO}} 3.5},
  doi = {10.5194/gmd-2016-174},
  language = {en},
  urldate = {2016-09-21},
  journal = {Geoscientific Model Development Discussions},
  author = {Bessi{\`e}res, Laurent and Leroux, St{\'e}phanie and Brankart, Jean-Michel and Molines, Jean-Marc and Moine, Marie-Pierre and Bouttier, Pierre-Antoine and Penduff, Thierry and Terray, Laurent and Barnier, Bernard and S{\'e}razin, Guillaume},
  month = sep,
  year = {2016},
  keywords = {chasm,easc2018},
  pages = {1--24},
  annote = {XIOS makes use of parallel file systems capabilities via the Netcdf4-HDF5 format, that allows both online data compression 28 and parallel I/O. Therefore, XIOS is used in ``multiple file'' mode where each XIOS instance writes a file for one stripe of the 29 global domain, yielding 40 files times 50 members for each variable and each time. At the end of each job, the 40 stripes are 30 recombined on-the-fly into global files},
  file = {Bessières et al. - 2016 - Development of a probabilistic ocean modelling sys.pdf:/Users/BNL28/zotero-storage/storage/U5XX6CWI/Bessières et al. - 2016 - Development of a probabilistic ocean modelling sys.pdf:application/pdf}
}

@misc{XIOS12,
  timestamp = {2018-04-04T14:07:37Z},
  address = {Hamburg},
  title = {The {{XML I}}/{{O Server}}},
  urldate = {2017-05-13},
  author = {Meurdesoif, Yann and Ozdoba, H. and Caubel, A. and Marti, O.},
  year = {2012},
  keywords = {chasm,easc2018,semsl_paper},
  file = {XIOS:/Users/BNL28/zotero-storage/storage/FG8QCUCE/wiki.html:text/html}
}

@article{HawSut09,
  timestamp = {2017-12-06T10:00:28Z},
  title = {The {{Potential}} to {{Narrow Uncertainty}} in {{Regional Climate Predictions}}},
  volume = {90},
  issn = {0003-0007, 1520-0477},
  doi = {10.1175/2009BAMS2607.1},
  language = {en},
  number = {8},
  urldate = {2017-12-06},
  journal = {Bulletin of the American Meteorological Society},
  author = {Hawkins, Ed and Sutton, Rowan},
  month = aug,
  year = {2009},
  keywords = {easc2018},
  pages = {1095--1107},
  file = {Hawkins and Sutton - 2009 - The Potential to Narrow Uncertainty in Regional Cl.pdf:/Users/BNL28/zotero-storage/storage/BET5K5HH/Hawkins and Sutton - 2009 - The Potential to Narrow Uncertainty in Regional Cl.pdf:application/pdf}
}

@article{YeaEA18,
  timestamp = {2018-04-06T10:10:19Z},
  title = {Predicting Near-Term Changes in the {{Earth System}}: {{A}} Large Ensemble of Initialized Decadal Prediction Simulations Using the {{Community Earth System Model}}},
  issn = {0003-0007},
  shorttitle = {Predicting Near-Term Changes in the {{Earth System}}},
  doi = {10.1175/BAMS-D-17-0098.1},
  abstract = {A new community data resource offers unprecedented statistical power for assessing hindcast accuracy and skill, quantifying the benefits associated with initialization, and exploring the probabilistic attributes of decadal predictions of climate and ocean biogeochemical fields.},
  urldate = {2018-03-19},
  journal = {Bulletin of the American Meteorological Society},
  author = {Yeager, S. G. and Danabasoglu, G. and Rosenbloom, N. and Strand, W. and Bates, S. and Meehl, G. and Karspeck, A. and Lindsay, K. and Long, M. C. and Teng, H. and Lovenduski, N. S.},
  month = mar,
  year = {2018},
  keywords = {decadal prediction,easc2018},
  annote = {Extracted Annotations (19/03/2018, 08:12:12)
"The tier 1 set of hindcasts required for basic participation in the Decadal Climate Prediction Project (DCPP) of CMIP6 calls for roughly 3,000 years of coupled model simulation (Boer et al., 2016)." (Yeager et al 2018:5)
"This figure is based on a set of 10-member ensembles initialized each year for the past 60 years and integrated forward for 5 years (60x10x5 = 3,000)." (Yeager et al 2018:5)
"The resource demand doubles if the hindcast length is extended to 10 years so that skill can be evaluated at decadal, as opposed to multi-annual, lead times. The significant cost of such experiments makes it difficult, if not wholly unfeasible, to systematically evaluate the sensitivity to poorly-constrained DP configuration choices such as (to highlight a few): the ensemble size; the method of ensemble generation; the annual start date; the number of start times; the initialization method; the number of initialized Earth system components (in addition to the ocean); and the component model resolution(s)." (Yeager et al 2018:5)
"Furthermore, the identification of skill enhancement due to initialization requires a complementary set of "uninitialized" (UI) historical simulations, which greatly adds to the expense of DP evaluation. Both the DP and UI ensembles should ostensibly be large enough so that the ensemble average operation effectively isolates the shared component of variance within the respective ensembles (Boer et al., 2013). The standard 10-member ensemble is probably insufficient for this purpose for many fields and regions of interest (Sienz et al., 2015), but is generally deemed adequate for 95 pragmatic reasons." (Yeager et al 2018:6)
"The CESM decadal prediction large ensemble (CESM-DP-LE) is comprised of 40-member ensembles initialized each November 1st between 1954 and 2015 (for a total of 62 start dates) and integrated for 122 months." (Yeager et al 2018:6)
"The CESM Large Ensemble (CESM-LE; Kay et al., 2015) is a highly successful community project that has accumulated a 40-member ensemble of historical/projection simulations spanning 1920-2100. The CESM-DP-LE was generated using the same code base, component model configurations, and historical and projected radiative forcings as in the CESM-LE." (Yeager et al 2018:6)
"The CESM-DP-LE is a rich, public data set that will support a broad spectrum of scientific research related to Earth System prediction. It comprises roughly 600 TB of climate data archived at temporal frequencies ranging from 6-hourly to annual from each of the CESM component models (ocean, atmosphere, sea ice, and land). It includes ocean biogeochemistry fields (as does the CESM-LE), and thus it permits~ exploration of the predictability of fundamental components of the ocean biosphere and carbon cycle." (Yeager et al 2018:7)
"The atmosphere~ component is the Community Atmosphere Model version 5 (CAM5; Hurrell et al., 2013)~ with a finite volume dynamical core at nominal 1$^\circ$ horizontal resolution and 30 vertical levels. The ocean component is version 2 of the Parallel Ocean Program (POP) run at nominal 1$^\circ$ horizontal resolution and 60 vertical levels (Danabasoglu et al., 2012). The sea ice model is version 4 of the Los Alamos National Laboratory (LANL) Community~ Ice Code (CICE4; Hunke and Lipscomb, 2008) and is run on the same horizontal grid as~ the ocean. The land model is version 4 of the Community Land Model (CLM4; Lawrence~ et al., 2011). The historical (up through 2005) and projected (from 2006 onwards)~ radiative forcings (including greenhouse and short-lived gases and aerosols) are identical to those used in CESM-LE. Following DCPP guidelines, historical volcanic aerosol forcings are applied in the DP experiments (Boer et al., 2016)." (Yeager et al 2018:8)
"The full field initialization necessitates a drift adjustment procedure prior to hindcast verification against observations. This is accomplished by transforming the raw DP~ output into anomalies relative to the climatological forecast for each lead time~ (Doblas-Reyes et al., 2013; 225 Kim et al., 2012) (Yeager et al 2018:11)
"The observational benchmarks used for hindcast evaluation in this study are as follows: ~ the UK Met Office EN4.2.1 gridded ocean temperature product for upper ocean heat ~ content (Good et al., 2013); the Extended Reconstructed Sea Surface Temperature ~ version 5 (ERSSTv5) data set from NOAA (Huang et al., 2017); the University of East~ Anglia Climatic Research Unit Time-Series version 4.00 (CRU-TS4.0) land surface ~ temperature and precipitation data set (Harris et al., 2014); and estimates of ocean net ~ primary productivity (NPP) generated from the Moderate Resolution Imaging ~ Spectroradiometer (MODIS, 2003-2015) using the Vertically Generalized Production ~ Model (Behrenfeld and Falkowski, 1997)." (Yeager et al 2018:13)
"With respect to predictions of regional precipitation over~ land in the two areas highlighted above, CESM-DP-LE does appear to exhibit a signal- ~ to-noise paradox similar to other DP systems insofar as significantly higher correlation ~ scores are obtained when verifying against observations than when verifying against ~ single-member model 'truth' (Fig. 8c,d). The ratio of predictable components (RPC; ~ computed as the ratio of median correlations: robs/rmodel) is 1.64 (2.05) for 40-member~ predictions of summer precipitation over Northern Europe (Sahel). This suggests that ~ CESM-DP-LE predictions of precipitation over land are underconfident, characterized by ~ unrealistically low signal-to-noise, and that the real world predictability may be higher ~ than what is implied by the model ensemble spread (Eade et al., 2014)." (Yeager et al 2018:25)},
  file = {Yeager et al. - 2018 - Predicting near-term changes in the Earth System .pdf:/Users/BNL28/zotero-storage/storage/SW77T8K9/Yeager et al. - 2018 - Predicting near-term changes in the Earth System .pdf:application/pdf;Snapshot:/Users/BNL28/zotero-storage/storage/RJ8PTB3X/BAMS-D-17-0098.html:text/html}
}

@article{Parker10,
  timestamp = {2018-04-06T09:23:59Z},
  series = {Special Issue: Modelling and Simulation in the Atmospheric and Climate Sciences},
  title = {Predicting Weather and Climate: {{Uncertainty}}, Ensembles and Probability},
  volume = {41},
  issn = {1355-2198},
  shorttitle = {Predicting Weather and Climate},
  doi = {10.1016/j.shpsb.2010.07.006},
  abstract = {Simulation-based weather and climate prediction now involves the use of methods that reflect a deep concern with uncertainty. These methods, known as ensemble prediction methods, produce multiple simulations for predictive periods of interest, using different initial conditions, parameter values and/or model structures. This paper provides a non-technical overview of current ensemble methods and considers how the results of studies employing these methods should be interpreted, paying special attention to probabilistic interpretations. A key conclusion is that, while complicated inductive arguments might be given for the trustworthiness of probabilistic weather forecasts obtained from ensemble studies, analogous arguments are out of reach in the case of long-term climate prediction. In light of this, the paper considers how predictive uncertainty should be conveyed to decision makers.},
  number = {3},
  urldate = {2018-04-06},
  journal = {Studies in History and Philosophy of Science Part B: Studies in History and Philosophy of Modern Physics},
  author = {Parker, Wendy S.},
  month = sep,
  year = {2010},
  keywords = {Climate,easc2018,Ensemble,Prediction,Simulation,Uncertainty,Weather},
  pages = {263--272},
  file = {ScienceDirect Full Text PDF:/Users/BNL28/zotero-storage/storage/NX235UZN/Parker - 2010 - Predicting weather and climate Uncertainty, ensem.pdf:application/pdf;ScienceDirect Snapshot:/Users/BNL28/zotero-storage/storage/M7VW6YZI/S1355219810000468.html:text/html}
}

@article{BuiEA05,
  timestamp = {2018-04-06T09:41:36Z},
  title = {A {{Comparison}} of the {{ECMWF}}, {{MSC}}, and {{NCEP Global Ensemble Prediction Systems}}},
  volume = {133},
  issn = {0027-0644, 1520-0493},
  doi = {10.1175/MWR2905.1},
  abstract = {The present paper summarizes the methodologies used at the European Centre for Medium-Range Weather Forecasts (ECMWF), the Meteorological Service of Canada (MSC), and the National Centers for Environmental Prediction (NCEP) to simulate the effect of initial and model uncertainties in ensemble forecasting. The characteristics of the three systems are compared for a 3-month period between May and July 2002. The main conclusions of the study are the following: \textbullet{} the performance of ensemble prediction systems strongly depends on the quality of the data assimilation system used to create the unperturbed (best) initial condition and the numerical model used to generate the forecasts; \textbullet{} a successful ensemble prediction system should simulate the effect of both initial and model-related uncertainties on forecast errors; and \textbullet{} for all three global systems, the spread of ensemble forecasts is insufficient to systematically capture reality, suggesting that none of them is able to simulate all sources of forecast uncertainty. The relative strengths and weaknesses of the three systems identified in this study can offer guidelines for the future development of ensemble forecasting techniques.},
  language = {en},
  number = {5},
  urldate = {2018-04-06},
  journal = {Monthly Weather Review},
  author = {Buizza, Roberto and Houtekamer, P. L. and Pellerin, Gerald and Toth, Zoltan and Zhu, Yuejian and Wei, Mozheng},
  month = may,
  year = {2005},
  keywords = {easc2018,ensembles},
  pages = {1076--1097},
  annote = {Extracted Annotations (06/04/2018, 10:40:27)
"Opinions diverge, however, on how to best describe the distribution of the initial errors and on how to subsequently sample that distribution. Three fairly different methods to generate an ensemble of initial conditions are currently in use at operational centers." (Buizza et al 2005:1076)
"Quantitative comparisons of the bred-vector-, singular-vector-, and perturbed-observation-based ensembles have been so far performed in simplified environments only." (Buizza et al 2005:1077)
"Forecast errors in real-world applications arise not only because of initial errors, but also because of the use of imperfect models. Representing forecast uncertainty related to the use of imperfect models is thought to be of an even greater challenge than simulating initial-value-related errors. As described in the next section, the three centers follow rather different approaches in this respect as well." (Buizza et al 2005:1077)
"The performance of the ensemble forecast systems is assessed considering three attributes of a forecasting system (Murphy 1973): statistical reliability (or consistency), resolution, and discrimination. Statistical reliability implies that a sample of forecasts is statistically indistinguishable from the corresponding sample of observations (or analysis fields). Reliability can often be improved through simple statistical postprocessing techniques. Though important for real-world applications, reliability of a forecast system in itself does not guarantee usefulness (e.g., a climatological forecast system, by definition, is perfectly reliable, yet has no forecast value). Statistical resolution reflects a forecast system's ability to distinguish between different future events in advance. Discrimination, which is the converse of resolution (Wilks 1995), reflects a system's ability to distinguish between the occurrence and nonoccurrence of forecast events. In case observed frequencies of forecast events monotonically increase with increasing forecast probabilities, resolution and discrimination\textemdash{}which are based on two different factorizations of the forecast/observed pair of events\textemdash{} convey the same information about forecast systems." (Buizza et al 2005:1085)
"Different measures, emphasizing different aspects of forecast performance, can be used to assess the statistical reliability, resolution, and discrimination of a forecast system. In this study, the performance of the three EPSs will be compared using a comprehensive set of standard ensemble and probabilistic forecast verification methods, including the pattern anomaly correlation (PAC), root-mean-square (rms) error, the Brier (1950) skill score, the outlier statistics (a measure of reliability), and the area under the relative operating characteristics (ROCs; a measure of discrimination; Mason 1982). The reader is referred to, for example, Stanski et al. (1989), Wilks (1995), Talagrand et al. (1997), and Toth et al. (2003) for a description of these scores." (Buizza et al 2005:1085)
"The above scores measure the quality of probabilistic forecasts of scalar quantities. In the context of this" (Buizza et al 2005:1085)
"study, one would also like to evaluate the relevance of perturbation patterns. The characteristics of the patterns could be very different for the three EPS systems. To investigate this, Wei and Toth (2003) designed a new measure called PECA. By evaluating how much of the error in a forecast can be explained by a single, or an optimal combination of ensemble perturbations, PECA ignores the magnitude of forecast errors that may dominate other verification measures. Therefore the PECA values shown in the next subsection may be helpful in attributing the ensemble performance results to differences in the quality of data assimilation, NWP modeling, and ensemble perturbation techniques at the three centers." (Buizza et al 2005:1085)
"In a chaotic system like the atmosphere, probabilistic information is recognized as the optimum format for weather forecasts both from a scientific and a user perspective. Ensemble forecasts are well suited to support the provision of such probabilistic information. In fact, ensembles not only improve forecast accuracy in a traditional sense (by reducing errors in the estimate of the first moment of the forecast probability distribution), but also offer a practical way of measuring casedependent variations in forecast uncertainty (by providing an estimate of the higher moments of the forecast probability density function)." (Buizza et al 2005:1095)
"The Brier score can be decomposed into its reliability, resolution, and uncertainty components:" (Buizza et al 2005:1096)
"The reliability term summarizes the calibration, or conditional bias, of the forecast. It consists of a weighted average of squared differences between the forecast probabilities and relative frequencies of the forecast event in each subsample. The resolution term summarizes the ability of the forecast to discern subsample forecast periods with different relative frequencies of the event. The forecast probabilities do not appear explicitly in this term, yet it still depends on the forecasts through the sorting of the events making up the subsample relative frequencies. The uncertainty term depends only on the sample climatological relative frequency and is unaffected by forecasts." (Buizza et al 2005:1096)
~},
  file = {Buizza et al. - 2005 - A Comparison of the ECMWF, MSC, and NCEP Global En.pdf:/Users/BNL28/zotero-storage/storage/9CQ7UCW9/Buizza et al. - 2005 - A Comparison of the ECMWF, MSC, and NCEP Global En.pdf:application/pdf}
}

@article{LawEA18,
  timestamp = {2018-04-06T10:13:51Z},
  title = {Crossing the {{Chasm}}: {{How}} to Develop Weather and Climate Models for next Generation Computers.},
  doi = {10.5194/gmd-2017-186},
  journal = {Geoscientific Model Development Discussions},
  author = {Lawrence, Bryan N. and Rezny, Mike and Budich, Reinhard and Bauer, Peter and Behrens, J{\"o}rg and Carter, Mick and Deconinck, Willem and Ford, Rupert and Maynard, Christopher and Mullerworth, Steve and Osuna, Carlos and Porter, Andy and Serradell, Kim and Valcke, Sophie and Wedi, Nils and Wilson, Simon},
  year = {2018},
  keywords = {bnl,easc2018,aces}
}


