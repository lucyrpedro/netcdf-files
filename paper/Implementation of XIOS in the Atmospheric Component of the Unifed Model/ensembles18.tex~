\documentclass[twocolumn, 5p, times]{elsarticle} 

\usepackage{amsmath}

%%% temporary while writing the paper, must go for submission:
\usepackage{color}
\newcommand{\editorial}[1]{{\it\color{red}#1}}
%%%%

\begin{document}

\title{Implementation of XIOS in the Atmospheric Component of the Unifed Model}

\author[ncas,met,comp]{Bryan Lawrence}
\author[ncas,met]{Grenville Lister}
\author[ncas,met]{Jeff Cole}
\author[epcc]{Rupert Nash}
\author[epcc]{Michele Weiland}

\address[ncas]{National Centre for Atmospheric Science, UK}
\address[met]{Department of Meteorology, University of Reading, UK}
\address[comp]{Department of Computer Science, University of Reading, UK}
\address[epcc]{EPCC, The University of Edinburgh, UK}

\begin{abstract}
Weather and climate science make heavy use of ensembles of model simulations to provide estimation of uncertainty arising from a range of causes (e.g. \cite{HawSut09}). Current practice is to write each ensemble member (simulation) out to disk as it is running, and carry out an ensemble analysis at the end of the simulation. Such analysis will include simple statistics, as well as detailed analysis of some ensemble members. However, as model resolutions increase (with more data per simulation), and ensemble sizes increase (more instances), the storage and analysis of this data is becoming prohibitively expensive --- many major weather and climate sites are looking at managing in excess of an exabyte of data within the next few years. This become problematic for an environment where we anticipate running such ensembles on exascale machines which may not themselves include local storage of sufficient size where data can be resident for long periods of analysis.

There are only two possible strategies to cope with this data deluge - data compression (including ``thinning'', that is the removal of data from the output) and in-flight analysis. We discuss here some first steps with the latter approach. We exploit the XML IO server (XIOS, \cite{XIOS12}) to manage the output from simulations and to carry out some initial analysis en-route to storage.  

We have achieved three specific ambitions: (1) We have adapted a current branch of the Met Office Unified Model to replace most of the diagnostic system with the XIOS.
(2) We have exploited a single executable MPI environment to run multiple UM instances with output sent to XIOS, and (3) We have demonstrated that simple ensemble statistics can be calculated in-flight, including both summary statistics of individual ensemble members, and cross-member statistics such as means and extremes.

With this ability, we can in principle avoid having all data needing to reside on fast disk when the ensemble simulation is complete. This would allow, for example, deployment on an exascale machine with burst-buffer migrating data directly to tape (or to the wide area network). 

There are some issues yet to be resolved. In particular, we need to manage the MPI context to explicitly mange errors propagating up from an ensemble member (an errant ensemble member could otherwise halt the execution), and we need to consider how to bring third party data into the XIOS context so that non-linear comparisons can be calculated and meaned at run time. Neither of these are expected to be very difficult, but they will involve further engineering.

In the longer-term, in-flight analysis will have to address some sort of steering where not all ensemble members are output for the entire duration of the simulation, but even this interim method will help with data management. It will be possible to identify ``interesting'' ensemble members from summary statistics, and keep them online for more detailed analysis, while less (initially) interesting ensemble members can be more rapidly migrated to colder storage for later analysis.
\end{abstract}

\begin{keyword}
ensemble
\end{keyword}

\maketitle

\section{Introduction}

As increasing computer power has become available, the weather and climate community have put effort into increasing the spatial resolution within the simulation domain, increasing the domain of simulation (spatial and/or temporal), expanded use of, or complexity of, data assimilation to improve initial conditions, and increasing the number of simulations within ``ensembles''. Such ensembles are collections of simulations which address the same problem, but where some key aspect of each simulation differs from the others. 

Ensembles typically vary along one or more of four axes: initialisation, boundary conditions, physical parameters, and modelling system (aka ``Model'').  Initialisation ensembles are the mainstay of numerical weather prediction - choosing the right set of initialisations is a science in and of itself \cite{BuiEA05}. Ensembles along the other dimensions are more normally used in climate science (e.g. the coupled model intercomparison projects such as CMIP6, \cite{MeeEA14}), but are also increasingly used at shorter timescales for weather related problems. In all cases, the ensembles are used to sample uncertainty - see \cite{HawSut09} for an example from climate science, and \cite{Parker10} for a more philosophical discussion of how ensembles are used to sample uncertainty.

Whatever the use, the workflows associated with ensembles are becoming problematic: when ensemble simulations are run in parallel on the same platform  they can swamp the available bandwidth to, and volume available at, local storage systems. When they are run sequentially, the volume issues remain, and workflow and queuing delays become new hurdles to surmount.
 Whether simulations are run sequentially or in parallel the current mode of usage is to write out all the data from each simulation and post-process to produce ``ensemble statistics'', and at many sites this can be difficult, especially for large, long, and high-resolution ensembles.   
 Managing the workflows has become an exercise in logistics \cite{MizEA09}. These ensembles are also a major contributor to the vast archives of data held at major weather and climate sites, and together with the cost of adequate disk, the cost of even tape archive is becoming problematic. 
 All these problems are expected to increase in the next few years, not only is there increasing scientific focus on large ensembles (e.g. \cite{YeaEA18}), problems with getting models ready for exascale \cite{LawEA18} will probably mean the first usage of exascale machines in weather and climate will be for large ensembles.

There are only two possible strategies to cope with this data deluge - data compression (including ``thinning'', that is the removal of data from the output) and in-flight analysis. In this paper we 
introduce some first steps with the latter approach. 
In particular, we demonstrate the use of a single executable climate ensemble system developed so that each member simulation shares access to an extra component - an IO server - which carries out data reductions before writing data to disk. 

Section \ref{context} discusses the climate model environment and introduces the particular IO server that is being used. Section \ref{software} introduces the software that was developed to support the ensemble system and section \ref{results} reports on our experiences with the system. Section \ref{related} discusses related work, and in section \ref{summary} we summarise our achievements and signpost some of our plans for further work.


\section{Context}
\label{context}

Climate modelling in the UK is predominantly done using variants of the Unified Model, running standalone in ``atmosphere only mode'' \citep{REFNEEDED}, or in coupled mode \citep{REFNEEDED}. The coupled variant is depicted in Figure \ref{coupled}: the UM atmosphere is coupled via the OASIS coupler \citep{REFNEEDED} to a NEMO \cite{REFNEEDED} ocean. NEMO uses the XIOS \cite{XIOS12} to write to disk, and the UM atmosphere either writes directly to disk, or uses an internal UM IO server.

The XIOS already has support for user configurable data reductions, and has already been used to manage ensembles \cite{BesEA16}, so was a natural target for investigating ensemble support.


\begin{figure}
	\centerline{
		\includegraphics[scale=0.5]{figures/xios_figures_a.pdf}
	}
	\caption{Schematic of the existing UK climate mode.}
	\label{coupled}
\end{figure}

\subsection{The Unified Model}

Devloped jointly by the Met Office, its partner organizations, and the academic climate and weather research community, the UM is the model chiefly used to run numerical simulations in the UK academic and research-centre environments. Much of this work is undertaken on ARCHER, accounting for ~2000 MAU annually. The model is run in a wide range of configurations varying from low-resolution, high-fidelity Earth System models (UKESM), to very high-resolution climate (PRIMAVERA), to ultra high-resolution process studies (PARACON). The UM is an MPI-OMP code written in FORTRAN, with a small amount of C. The UM has very few software dependencies, principally the communications library (GCOM), which is simply a wrapper for MPL, and, depending on precisely how the model is configured, possible dependencies on NetCDF and HDF5. The UM is managed through a workflow system, which combines a model configuration system (Rose) and workflow scheduling engine (Cylc).

\subsubsection{Climate Configuration}
While the UM can be configured with time varying sea-surface acillary fields to mimic fully interactive atmosphere-ocean coupling, it is more commonly the case that a fully coupled configuration is employed. Figure \ref{coupled} outlines the components of the coupled model - the UM (atmosphere) and NEMO (ocean) run asynchronously with a defined coupling frequency. Fields are exchanged between the two componenets by meaans of the OASIS coupler, which performs the required conservative regridding between UM (lat-long) and NEMO (tri-polar) grids. The UM currently manages its output through its proprietary IO server scheme and NEMO uses XIOS to output diagnostics (prognostics are handled differently.)

\begin{figure}
        \includegraphics[width=\columnwidth]{figures/xios_figures_c.pdf}
        \caption{The Unified Model Configuration and output data flow (for an atmosphere only instance): configuration files define the domain decomposition (the number of processing engines, PEs) and the outputs required. Output is written to ``STASH'' and then, in the IO server configurtion, written to multiple IO server instances, which each write to disk. One MPI context (shown in yellow) covers the entire activity.
                Control flow shown in black, data flow in red.
}
\label{uma-fig}
\end{figure}

\subsubsection{Current UM IO scheme}
The UM diagnostics system, commonly referred to as STASH, has been developed over 25 years into a highly configurable and versatile scheme for extracting and filtering UM fields. Recent advances have seen the successful development of CF-NetCDF capability circumventing the writing data in UM format. Diagnostics are made available for selection through the Rose GUI, where in addition, they can be configured for temporal and spatial processing. Rose generates FORTAN namelists from user choices - the workflow is sketched in Figure \ref{uma-fig} - read at runtime to control model execution.
The UM IO scheme is a client-server model; diagnostics are processed and buffered on the compute PEs then moved to the servers (UM-IOS in Figure \ref{uma-fig}) to be written to disc. The number and placement of server processes is configurable.

\subsubsection{Resolution and Performance}
Climate models typically run at low or high resolution. Low resolution is designated as N96 with 96x2 zonal grid points for resolution of 135km at the equator; high resolution is designated N512, which translates to 25km on the equator. We are currently running N1280 (5km) and developing N2560 (2.5km).

Figure \ref{scaling-um-fig} shows the results of some scaling runs for a high-resolution (N512) atmosphere-only climate model, which is essentially the model used throughout this work, on ARCHER and NEXCS (part of the MO Cray XC40), with and without IO. IO was managed through the UM IO servers (see Figure \ref{uma-fig}), which implement achynchronous IO to write data in the preprietary UM data format.  The figure also shows scaling behaviour for a low-resoulution model which includes an expensive atmospheric chemistry scheme. All runs were performed with two OMP threads (required for the UM IO server). The N512 model scales reasonably well out to 15000 cores both and without IO - changes to the IO profile can have significant impact on performance through increased stalling resulting from inappropriate buffering and/or simply overmatcing the machine IO bandwidth (recall that ARCHER is a very heavily loaded machine).) 


\begin{figure}
        \includegraphics[width=\columnwidth]{figures/Cores.pdf}
        \caption{UM atmosphere strong scaling for a high resolution (N512) model with and without IO, and for a low resolution (N96) model with atmospheric chemistry included.}
        \label{scaling-um-fig} 
\end{figure}



\subsection{XIOS}
\editorial{words about XIOS, XML, spatial and temporal filtering, reductions (CMIP6 usage)}
XIOS (XML IO Server) is a software system devloped at IPSL (Institute Pierre Simon Laplace) through IS-ENES (Infrastructure for the European Network of Earth System Modelling) (https://is.enes.org/), CONVERGENCE, ICOMEX, and ESiWACE. XIOS is organized around a heirarchical description of its data through an external XML file providing a simple interface for the user. It provides dedicated servers for asynchronous output to overlap with computation, parallel I/O for single file output and posssible performance improvement and simplified down stream data workflow, in addition to multiple file (one per IO server) output. XIOS also offers the prospect of \textit{in  situ} data analysis (our use of the phrase \textit{in flight} is based on this potential).  

XIOS is ~90,000 lines of C++; it is open source software (under CeCiLL licence) and avaialable at http://forge.ipsl.jussieu.fr/ioserver. The XIOS build system is based on FCM (https://www.metoffice.gov.uk/research/collaboration/fcm) with support for Intel and Cray compilers. We have built and run XIOS with Intel and Cray - note compliation with Cray is slow. Our use of XIOS is based primarily on revision 1404.

XIOS is based on a client-server arctitecture, whereby each compute processor interacts with an XIOS client to expose agreed data fields through a mininmalist interface. The set of fields to be exposed is defined by entries in the XML file. A simple FORTRAN call is all that the model needs to do to offload data to XIOS, thus:

\begin{verbatim}
call xios_send_field("field_id", field)
\end{verbatim}


where \texttt{"field\_id"} is a reference to the XML descripion of the field which includes grid and domain information, and \texttt{field} is the address of the field data.


\subsubsection{XML}
The contents of the XML file are distinguished by \texttt{context}, which may represent a model or a component of a model. In the following XML snippet, we have defined the \texttt{atmosphere} context with associated XML elements referencing axis (typically the vertical or pressure axis in a climate model), domain (the horizontal structure), grid (combinations of domains and axes), field (specifying a set of \texttt{field\_id}s and their associated grids (a given \texttt{field\_id} can be output on multiple grids)), and file (specifying  output frequency, filename, other purely file-related criteria.) The XML file will typically comprise of several contexts, each associated with a logically appropriate model or component.  

\begin{verbatim}
<simulation>
  <context id="atmospere" >
    <axis_definition   src="./axis_def.xml"  />
    <domain_definition src="./domain_def.xml"/>
    <grid_definition   src="./grid_def.xml"  />
    <field_definition  src="./field_def.xml" />
    <file_definition   src="./file_def.xml"  />
  </context>
  <context id="other" >
  ....
  </context>
</simulation>
\end{verbatim}

Judging from our experience with NEMO, the XML file is a very slowly changing object, ie model output is defined once and hardly changes over time. Use of XIOS in the UM research environment will require a much more quickly evolving XML configuration, and the abilty to easily modify it through methods incorporated into the well established UM model configuration infrastructure - this is a must to enure its accepatance by the UM community. 


\section{Software Structure}
\label{software}

There were four distinct software activities required to develop our ensemble system based on XIOS:
\begin{enumerate}
\item The XIOS system had to be inserted alongside the existing diagnostic system (we did not in these experiments completely replace the existing diagnostic system) 
\item We had to work out how to get XIOS to deliver ensemble statstics,
\item We had work out how to get XIOS to control the ensemble,
\item We had to develop methods of configuring the model ensemble system to request the required diagnostics.
\end{enumerate}


\begin{figure}
	\centerline{
		\includegraphics[scale=0.5]{figures/xios_config.pdf}
	}
	\caption{Maintaining the look and feel of UM diagnostics: the Rose GUI is used to configure both the UM atmosphere via namelists, and the XIOS output. The UM reads the XML files and integrates them with the STASH system and sends the fields to the XIOS server.} \label{fig-xconfig}
\end{figure}




STASH output is configured to go through XIOS - currently any field available on a timestep. We are restricted to single threading currently.

\subsection{XIOS in the UM}

Given the maturity of the Rose system and its familarity in the UM user community, it was essential that we did not develop a new diagnostic interface. Our goal was to maintian the look and feel of the current system to enable user easy uptake of the new XIOS functionality. Figure \ref{fig-xconfig} shows the steps to achieve our objective. The diagnostic choices must be translated into an XML format to be read by XIOS and the UM itself must be modified to hand off data to XIOS. 
\subsubsection{STASH to XML utility}
\begin{figure}
	\centerline{
	\includegraphics[scale=0.5]{figures/stash2xiosxml.pdf}}
	\caption {
                Work flow for creating XML files from STASH requests. The familiar Rose GUI isolates the user from details about XIOS XML. \texttt{stash2xiosxml} reads configuration files generated by Rose to create an XML description of the diagnostic output.
		}\label{stash2xiosxml}
\end{figure}
UM diagnostics are selected and configured through a Rose interface. The user selects a diagnostic and associates it with three "profiles", namely, time, domain, and usage. The time and domain profiles represent specifications for temporal and spatial processing for the selected diagnostic (time meaning or accumulation, spatial meaning or subspacing, for example); the usage profile, which, in combination with an output stream, determines the file structure for the diagnostic. We wished to maintain the familiar diagnostics interface for use with XIOS; this was achieved through the addition of XIOS output streams to complement regular UM output streams, and the developmemt of a utility to identify diagnostics destined for XIOS and translate their traditional STASH description into XML. 

We have developed \texttt{stash2xiosxml} to make the translation. 
\texttt{stash2xiosxml} is a Python utility which loads Rose suite configuration files into a heirarchical data structure, extracts and collates information relevant for XIOS diagnostics and writes the XML. Figure \ref{stash2xiosxml} is a schematic of the steps involved in XML creation; Rose creates possibly many configuration files which are inputs to \texttt{stash2xiosxml}; the XIOS standard XML file iodef.xml is output along with its subcomponents.


\subsubsection{New code in the UM} 
Files added to the UM named with the prefix \texttt{um\_xios} represent new code. The additional files are located in \texttt{/um/src/control/xios} and comprise approximately 3000 lines code.

\begin{figure}
	\centerline{
	\includegraphics[scale=0.5]{figures/UM-XIOS-code.pdf}}
	\caption {
                Areas in the UM where use of XIOS impacts the code. Diagnostics are intercepted in the routine stash. All other UM IO (logging, checkpointing, and diagnostics not directed to XIOS) is handled in the traditional manner.
		}\label{um-xios-code}
\end{figure}


\subsubsection{Build system and job submission} 
Minor changes were made to the Rose GUI and UM build configuration files to accommodate the use of the XIOS library. 
Minor changes were made to the Rose GUI and the scripts which generate the job submission file to enable the launch of the UM and XIOS in MPMD mode. 




\subsection{XIOS ensemble}
\editorial{XML for reductions, Planets test case}
\subsubsection{UM code for an ensemble}
\editorial{comms splitting, running in the right place (work), ensemble axis, single context}
\begin{figure}
	\centerline{
	\includegraphics[scale=0.5]{figures/xios_figures_b.pdf}}
	\caption {
		Key control and data flow concepts for XIOS control of a UM ensemble. The XIOS controls
		the output of each UM instance by exploiting
		XIOS clients within each process of each UM instance. Output from the clients goes through
		XIOS server instances to disk. 
		\editorial{Finish when understood}
		}\label{fig-xios-layout}
\end{figure}

\subsubsection{Rose ensemble configuration}
\editorial{suite development, ensemble setup, override files, zoom, single member output(multi-context), ensemble XML}



\subsection{Controlling the Ensemble}

The global communicator is split in a call to xios\_initialise which returns a local client communicator and a writing communicator. \editorial{Is that it?} The client communicator is further split for each ensemble member. Individual ensemble member models run on their own model communicators, read and write input, logging files, check points... in their own separate spaces (not through XIOS). 

\subsection{Ensemble Statistics}



\subsection{Configuring Ensemble Output}

\clearpage
\section{Results}
\label{results}


\subsection{Single UM}
\editorial{N96, N512, direct output (no IO servers), equivalent XIOS - asynchronicity}

\subsection{UM ensembles}
\editorial{N96, N512, all output/reductions only - realistic data rates}

\begin{table}
	\begin{center}
	\begin{tabular}{|l|r|r|r|}
		\hline\hline
		Resolution & \multicolumn{3}{c|}{Ensemble Size} \\
		 & 100 & 20 & 10 \\ \hline
		N96 & 9600 & & \\
		N512 & & & 54000 \\
		\hline\hline
	\end{tabular}
	\caption{Maximum core counts for the largest ensembles}
	\end{center}
\end{table}


\begin{figure}
	\centerline{\editorial{ProvisionalResults}} \includegraphics[width=\columnwidth]{figures/results.pdf}
	\caption{\editorial{We want to have results from 10, 20, 50, 100 member N96 and  1,2,5 and 10 member N512}Scaling curves for ensemble writing using N96 and N512 ensembles for single file output and multiple file output. \editorial{Need to add flat lines for perfect performance.}}
\end{figure}

\editorial {In discussion we want to sum the total times of execution of the ensemble and the aggregation using CF python. We want to explain why we haven't tunded to the file system for now.}

\section{Related Work}
\label{related}

\section{Summary and Further Work}
\label{summary}

We have achieved three specific ambitions: (1) We have adapted a current branch of the Met Office Unified Model to replace most of the diagnostic system with the XIOS.
(2) We have exploited a single executable MPI environment to run multiple UM instances with output sent to XIOS, and (3) We have demonstrated that simple ensemble statistics can be calculated in-flight, including both summary statistics of individual ensemble members, and cross-member statistics such as means and extremes.

\section*{Acknowledgements}

DSE, NCAS, ESIWACE

\section*{References}

\bibliographystyle{elsarticle-num}
\bibliography{ensembles}

\end{document}
